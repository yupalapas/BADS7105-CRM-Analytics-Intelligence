# -*- coding: utf-8 -*-
"""‡πáHomework 11 - Customer Voice Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zs6eJHeiXho1c-ssZlqxcs-VbMPb2x4f
"""

!pip install --upgrade pythainlp
!pip install pyLDAvis
!pip install -U pandas-profiling
!pip install sefr_cut

import pandas as pd
import pythainlp
import sefr_cut
import gensim
# import pyLDAvis.gensim
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
pyLDAvis.enable_notebook()
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

df = pd.read_csv('CustomerReviews.csv')

df.head(5)

"""Tokenize Words"""

stopwords = list(pythainlp.corpus.thai_stopwords())
removed_words = [' ', '  ', '\n','\n\n', '‡∏£‡πâ‡∏≤‡∏ô', '(', ')' , '           ','‚Äì', '!', '!!','-','/','+','üòÜ','ü§£','"','','%','\u200b','::']
screening_words = stopwords + removed_words

sefr_cut.load_model(engine='ws1000')
def tokenize_with_space(sentence):
  merged = ''
  # words = pythainlp.word_tokenize(str(sentence), engine='newmm')
  words = sefr_cut.tokenize(sentence)
  for word in words[0]:
    if word not in screening_words:
      word = word.rstrip("\n")
      word = word.rstrip("\u200b")
      if word is None or word == ' ' or word == '' or word ==':':
        continue
      else: 
        merged = merged + ',' + word
    elif word is None:
      continue
  return merged[1:]
  
  # return words

df['Review_tokenized'] = df['Review'].apply(lambda x: tokenize_with_space(x))

df.info()

df['Review_tokenized'].iloc[10]

df.tail()

"""
Create Dictionary"""

# documents = df['Review_tokenized'].to_list()
documents = df['Review_tokenized']
texts = [[text for text in doc.split(',')] for doc in documents]
dictionary = gensim.corpora.Dictionary(texts)

print(dictionary.token2id.keys())

gensim_corpus = [dictionary.doc2bow(text, allow_update=True) for text in texts]
word_frequencies = [[(dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]

"""Topic Modelling"""

# Commented out IPython magic to ensure Python compatibility.
num_topics = 5
chunksize = 4000 # size of the doc looked at every pass
passes = 20 # number of passes through documents
iterations = 100
eval_every = 1  # Don't evaluate model perplexity, takes too much time.

# Make a index to word dictionary.
temp = dictionary[0]  # This is only to "load" the dictionary.
id2word = dictionary.id2token

# %time model = gensim.models.LdaModel(corpus=gensim_corpus, id2word=id2word, chunksize=chunksize, \
                       alpha='auto', eta='auto', \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)

# pyLDAvis.gensim.prepare(model, gensim_corpus, dictionary)
gensimvis.prepare(model, gensim_corpus, dictionary)

model.show_topic(3)

df['topics'] = df['Review_tokenized'].apply(lambda x: model.get_document_topics(dictionary.doc2bow(x.split(',')))[0][0])
df['score'] = df['Review_tokenized'].apply(lambda x: model.get_document_topics(dictionary.doc2bow(x.split(',')))[0][1])
df['topic_name'] = df['Review_tokenized'].apply(lambda x: model.show_topic(model.get_document_topics(dictionary.doc2bow(x.split(',')))[0][0]))
df['topic_name2'] = df['topic_name'].apply(lambda x:convert_to_topic(x))

df[['Restaurant', 'Review', 'topics','topic_name2','score']]

"""Interpretation

‡∏û‡∏ö‡∏ß‡πà‡∏≤ Topic Name ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ 5 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á 

Topic 0 ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á ‡∏î‡∏µ ‡∏ô‡πâ‡∏≥ ‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏ó‡∏≤‡∏ô ‡∏´‡∏ß‡∏≤‡∏ô ‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡∏Å‡∏¥‡∏ô ‡∏Ñ‡∏∏‡πâ‡∏°

Topic 1 ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á ‡∏Å‡∏¥‡∏ô ‡∏î‡∏µ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡∏Ñ‡∏ô ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏ó‡∏≤‡∏ô ‡∏ô‡πâ‡∏≥ ‡∏£‡∏≤‡∏Ñ‡∏≤ ‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡∏ö‡∏≤‡∏ó

Topic 2 ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡∏ô‡πâ‡∏≥ ‡∏ó‡∏≤‡∏ô ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏ã‡∏∏‡∏õ ‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡∏î‡∏µ ‡∏´‡∏ß‡∏≤‡∏ô ‡∏Å‡∏¥‡∏ô

Topic 3 ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á ‡∏ó‡∏≤‡∏ô ‡∏î‡∏µ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏£‡∏≤‡∏Ñ‡∏≤ ‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏ô‡πâ‡∏≥ ‡∏ö‡∏≤‡∏ó ‡∏™‡∏≤‡∏Ç‡∏≤ ‡∏≠‡∏£‡πà‡∏≠‡∏¢

Topic 4 ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á ‡∏Å‡∏¥‡∏ô ‡∏î‡∏µ ‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏ó‡∏≤‡∏ô ‡∏Ñ‡∏ô ‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡∏™‡∏î ‡∏ô‡πâ‡∏≥ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å



‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Topic ‡∏Ñ‡∏ß‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤

Topic 0 ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ñ‡∏∏‡πâ‡∏° ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏î‡∏µ ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡∏£‡πà‡∏≠‡∏¢ 

Topic 1 ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏µ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏≠‡∏£‡πà‡∏≠‡∏¢

Topic 2 ‡∏ã‡∏∏‡∏õ‡∏´‡∏ß‡∏≤‡∏ô‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏î‡∏µ

Topic 3 ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏î‡∏µ ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ô‡πâ‡∏≥‡∏Ñ‡∏∏‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤

Topic 4 ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏™‡∏î ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏î‡∏µ ‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏≤‡∏ô‡πÑ‡∏î‡πâ
"""